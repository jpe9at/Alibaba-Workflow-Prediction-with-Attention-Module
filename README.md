This is part of a comparison project between different architectures for time series predictions. The three architectures that are being compared are:

Plain LSTM: https://github.com/jpe9at/Alibaba-Workflow-Prediction-with-LSTM
Encoder Decoder LSTM: https://github.com/jpe9at/Alibaba-Workflow-Prediction-with-Encoder-Decoder-LSTM/
Encoder Decoder LSTM with Attention: https://github.com/jpe9at/Alibaba-Workflow-Prediction-with-Attention-Module

For a full description and results, see: https://github.com/jpe9at/Alibaba-Workflow-Prediction-with-Attention-Module/blob/main/Comparison%20of%20workload%20predictors.pdf
